<!DOCTYPE html>
<html>
  <head>
    <title>PSA</title>
    <meta charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="core/fonts/mono.css"> 
    <link rel="stylesheet" type="text/css" href="core/css/animate.css"> 
    <link rel="stylesheet" type="text/css" href="core/css/cinescript.css"> 
    <link rel="stylesheet" type="text/css" href="core/css/style_core.css"> 
    <link rel="stylesheet" type="text/css" href="core/css/mermaid.css"> 
    <link rel="stylesheet" type="text/css" href="core/css/gitgraph.css"> 
    <link rel="stylesheet" type="text/css" href="core/css/style_ensiie.css"> 
    <link rel="stylesheet" type="text/css" href="core/css/katex.css"> 
  </head>
  <body>
    <textarea id="source" readonly>

layout: true
class: animated fadeIn middle numbers

.footnote[
`PSA` - N. Dubray - ENSIIE - 2019 - [:book:](../index.html)
]

---

# `MPI`

## Message Passing Interface (`MPI`)  

:arrow_right: An Application Programming Interface (API) allowing different processes to communicate.  

:+: The most widely used technique for distributed HPC.  
:+: `MPI` is available on **every HPC hardware**.  
:+: `MPI` programs are **scalable**.  
:+: `MPI` standard defines `C` and `Fortran` interfaces.

---

# `MPI` - versions

## Version `MPI-1.x`

:+: Point-to-point communications.  
:+: Collective communications.  
:+: Communicators.  
:+: Derived types.  
:+: Topologies.  
:+: `C` and `Fortran` interfaces.

## Version `MPI-2`

:+: Dynamical process management.  
:+: Parallel I/O.  
:+: One-sided communications (RMA).  
:+: Collective communications between intercommunicators.  
:+: `C++` and `Fortran95` interfaces.  

---

# `MPI` - `Python` ?

.hcenter[:arrow_right: Is it possible to use `MPI` from `Python` programs ?]

---

# `mpi4py` -  presentation

.noflex[
.rightf.shadow.w30[![](images/salvation.png)]

## `MPI` for `Python` !

:arrow_right: Full-featured `MPI` bindings for `Python`.  
:+: Following the standard `MPI-2` specifications.  
:+: **Almost all `MPI-2` functions** are supported.  
:+: Works with `MPI-1` and `MPI-2` implementations.  
:bulb: Can be used with `logging` using a [custom handler](https://groups.google.com/forum/#!topic/mpi4py/SaNzc8bdj6U).  

.vspace[]

## Other `MPI` `Python` bindings/implementations

* [`pyMPI`](https://sourceforge.net/projects/pympi/): `MPI`-enabled `Python` interpreter.
* [`pypar`](https://github.com/daleroberts/pypar): `Python` module.
* `MPI` submodule in [`ScientificPython`](https://sourcesup.renater.fr/projects/scientific-py/).
* Bindings in `Boost::MPI` (`C++`).
* ...
]

---

# `mpi4py` - communicators

## Communicators

:arrow_right: A **communicator** defines a set of processes which can communicate.  
:+: **Broadcast operations** allow all processes of a given communicator to communicate simultaneously.  
:+: Only **processes of the same communicator** can communicate.  

.vspace[]

## Standard pre-defined communicators

* `COMM_WORLD`: all processes created by `mpiexec`.
* `COMM_SELF`: only the current process.
* `COMM_NONE`: empty set.

.vspace[]

:bulb: Most of the time, only `COMM_WORLD` is needed.

---

# `mpi4py` - hello world

:arrow_right: Get the **rank** of a process in a given **communicator** with `Get_rank()`.

## .hcenter[\[mpi4py/hello_world.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

print("Hello world ! My rank: %d" % (rank))
```

## .hcenter[Shell session]
```shell
*$ mpiexec -np 5 mpi4py/hello_world.py
Hello world ! My rank: 0
Hello world ! My rank: 4
Hello world ! My rank: 1
Hello world ! My rank: 2
Hello world ! My rank: 3
```

:arrow_right: "`mpiexec -np 5 prog`" launches 5 instances of `prog` (possibly on different nodes).  
:warning: A **race condition** in the standard output causes the **random order** of messages.  
:warning: Depending on the `MPI` implementation, standard output can be garbled or not.  

---

# `mpi4py` - custom communicator

:arrow_right: Create a custom communicator with `Split()`.

## .hcenter[\[mpi4py/custom_communicator.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

comm2 = comm.Split(color = rank / 4)
rank2 = comm2.Get_rank()

print('global rank: %02d local rank: %02d' % (rank, rank2))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/custom_communicator.py | sort -n
global rank: 00 local rank: 00
global rank: 01 local rank: 01
global rank: 02 local rank: 02
global rank: 03 local rank: 03
global rank: 04 local rank: 00
global rank: 05 local rank: 01
global rank: 06 local rank: 02
global rank: 07 local rank: 03
global rank: 08 local rank: 00
global rank: 09 local rank: 01
```

:arrow_right: `Split()` creates new communicators for processes sharing the same "color".

---

# `mpi4py` - communications

:arrow_right: `mpi4py` provides two classes of communication methods:

.vspace[]

## Sending and receiving generic `Python` objects
:arrow_right: Method names start with a **lowercase letter** (`send()`, `recv()`, `isend()`, `irecv()`...).  
:+: Allows to send almost all types of `Python` objects (using `pickle`).  
:warning: Does not reach the same performances as `C`.  

.vspace[]

## Sending and receiving `C`-like arrays
:arrow_right: Method names start with an **uppercase letter** (`Send()`, `Recv()`, `Isend()`, `Irecv()`...).  
:warning: Can only send `C`-type arrays (`numpy` arrays).  
:+: Can reach the same performances as `C`.  

---

# `mpi4py` - blocking / non-blocking

## Blocking communication methods
:arrow_right: **`Blocking`** communication methods **wait** until the communication has completed.  
:+: These methods ensure that a communication has completed.  
:+: Using blocking communication methods often leads to **simpler code and logic**.  
:warning: Waiting time could be used to do something else.  

.vspace[]

## Non-blocking communication methods
:arrow_right: **`Non-blocking`** communication methods **return immediately**.  
:arrow_right: **`Non-blocking`** communication method names start with `I` or `i`.  
:warning: It is **up to the user** of these methods to check that a communication has completed.  
:warning: Using non-blocking communication methods requires more **complex code and logic**.  
:+: These methods allow **to do something instead of waiting** for a communication to complete.  
:+: There are some methods to **wait**, **test** and **cancel** a non-blocking communication.

---

# `mpi4py` - p2p blocking `Python` object

:arrow_right: Process 0 sends a `Python` object to process 1.

## .hcenter[\[mpi4py/p2p_blocking_python.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
  data = 'Hello COMM_WORLD !'
* comm.send(data, dest = 1)
elif rank == 1:
* data = comm.recv(source = 0)
  print("Received '%s'" % (data))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_blocking_python.py
Received 'Hello COMM_WORLD !'
```

---

# `mpi4py` - p2p non-blocking `Python` object

:arrow_right: Process 0 sends a `Python` object to process 1 in a non-blocking way.

## .hcenter[\[mpi4py/p2p_nonblocking_python.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
  data = 'Hello COMM_WORLD !'
* req = comm.isend(data, dest = 1)
* req.wait()
elif rank == 1:
* req = comm.irecv(source = 0)
* data = req.wait()
  print("Received '%s'" % (data))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_nonblocking_python.py
Received 'Hello COMM_WORLD !'
```

:arrow_right: This code is equivalent to `mpi4py/p2p_blocking_python.py`.

---

# `mpi4py` - p2p non-blocking `Python` object (v2)

## .hcenter[\[mpi4py/p2p_nonblocking_python_v2.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import time

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
  time.sleep(0.3)
  data = 'Hello COMM_WORLD !'
  req = comm.send(data, dest = 1)
elif rank == 1:
  req = comm.irecv(source = 0)
* flag, data = req.test()
* while not flag:
*   print("waiting (irecv)")
*   time.sleep(0.1)
*   flag, data = req.test()
  print("Received '%s'" % (data))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_nonblocking_python_v2.py
waiting (irecv)
waiting (irecv)
waiting (irecv)
Received 'Hello COMM_WORLD !'
```

---

# `mpi4py` - p2p non-blocking `Python` object (v3)

## .hcenter[\[mpi4py/p2p_nonblocking_python_v3.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import time

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
  time.sleep(0.3)
  data = 'Hello COMM_WORLD !'
  req = comm.send(data, dest = 1)
elif rank == 1:
  req = comm.irecv(source = 0)
  flag, data = req.test()
* if not flag:
*   req.Cancel()
*   print("cancelled")
  req = comm.irecv(source = 0)
  flag, data = req.test()
  while not flag:
    print("waiting (irecv)")
    time.sleep(0.5)
    flag, data = req.test()
  print("Received '%s'" % (data))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_nonblocking_python_v3.py
cancelled
waiting (irecv)
Received 'Hello COMM_WORLD !'
```

---

class: top

# `mpi4py` - `MPI.ANY_SOURCE`

## .hcenter[\[mpi4py/source.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import time
import random

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

if rank == 0:
  for i in range(1, size):
*   data = comm.recv(source = i)
    print(data)
  for i in range(1, size):
*   data = comm.recv(source = MPI.ANY_SOURCE)
    print(data)
else:
  data = 'Hello from rank %d' % (rank)
* comm.send(data, dest = 0)
  time.sleep(random.random())
* comm.send(data, dest = 0)```

--

## .hcenter[Shell session]
```shell
$ mpiexec -np 4 mpi4py/source.py
Hello from rank 1
Hello from rank 2
Hello from rank 3
Hello from rank 2
Hello from rank 1
Hello from rank 3
```

---

# `mpi4py` - p2p blocking `numpy` array

## .hcenter[\[mpi4py/p2p_blocking_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# passing MPI datatypes explicitly
if rank == 0:
  data = np.arange(1000, dtype = 'i')
* comm.Send([data, MPI.INT], dest = 1)
  print("send: %f" % (np.linalg.norm(data)))
elif rank == 1:
  data = np.empty(1000, dtype = 'i')
* comm.Recv([data, MPI.INT], source = 0)
  print("recv: %f" % (np.linalg.norm(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_blocking_numpy.py
send: 18243.724949
recv: 18243.724949
```

---

# `mpi4py` - p2p blocking `numpy` array (v2)

:arrow_right: The data type can be infered automatically.

## .hcenter[\[mpi4py/p2p_blocking_numpy_v2.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# automatic MPI datatype discovery
if rank == 0:
  data = np.arange(100, dtype = np.float64)
* comm.Send(data, dest = 1)
  print("send: %f" % (np.linalg.norm(data)))
elif rank == 1:
  data = np.empty(100, dtype = np.float64)
* comm.Recv(data, source = 0)
  print("recv: %f" % (np.linalg.norm(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_blocking_numpy_v2.py
send: 573.018324
recv: 573.018324
```

---

# `mpi4py` - p2p non-blocking `numpy` array

## .hcenter[\[mpi4py/p2p_nonblocking_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# automatic MPI datatype discovery
if rank == 0:
  data = np.arange(100, dtype = np.float64)
  comm.Send(data, dest = 1)
  print("send: %f" % (np.linalg.norm(data)))
elif rank == 1:
  data = np.empty(100, dtype = np.float64)
* req = comm.Irecv(data, source = 0)
* req.Wait()
  print("recv: %f" % (np.linalg.norm(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_nonblocking_numpy.py
send: 573.018324
recv: 573.018324
```

---

# `mpi4py` - p2p non-blocking `numpy` array (v2)

## .hcenter[\[mpi4py/p2p_nonblocking_numpy_v2.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np
import time

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# automatic MPI datatype discovery
if rank == 0:
  data = np.arange(100, dtype = np.float64)
  time.sleep(0.3)
  comm.Send(data, dest = 1)
  print("send: %f" % (np.linalg.norm(data)))
elif rank == 1:
  data = np.empty(100, dtype = np.float64)
* req = comm.Irecv(data, source = 0)
* while not req.Test():
*   print("waiting (irecv)")
*   time.sleep(0.1)
  print("recv: %f" % (np.linalg.norm(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_nonblocking_numpy_v2.py
waiting (irecv)
waiting (irecv)
waiting (irecv)
waiting (irecv)
send: 573.018324
recv: 573.018324
```

---

# `mpi4py` - p2p non-blocking `numpy` array (v3)

## .hcenter[\[mpi4py/p2p_nonblocking_numpy_v3.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np
import time

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# automatic MPI datatype discovery
if rank == 0:
  data = np.arange(100, dtype = np.float64)
  time.sleep(0.2)
  comm.Send(data, dest = 1)
  print("send: %f" % (np.linalg.norm(data)))
elif rank == 1:
  data = np.empty(100, dtype = np.float64)
  req = comm.Irecv(data, source = 0)
* if not req.Test():
*   req.Cancel()
*   print("cancelled")
    req = comm.Irecv(data, source = 0)
  while not req.Test():
    print("waiting (irecv)")
    time.sleep(0.1)
  print("recv: %f" % (np.linalg.norm(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 2 mpi4py/p2p_nonblocking_numpy_v3.py
cancelled
waiting (irecv)
waiting (irecv)
send: 573.018324
recv: 573.018324
```

---

# `mpi4py` - collective communications

:arrow_right: `MPI` provides several methods to allow 2+ processes to communicate "simultaneously".  
:warning: These methods exist in **blocking** mode only (except `Barrier()`).  

## Main `MPI` collective communication methods
1. **Barrier** synchronization.
1. **Broadcast** data: one-to-many, same data.
1. **Gather** data: many-to-one, individual data.
1. **Scatter** data: one-to-many, individual data.
1. **Reduction** operations: many-to-one, individual data **with reduction operation** (sum, min, max...).

## Methods using generic `Python` objects
:arrow_right: They start with a **lowercase letter**.  
:warning: Reduction operations using `Python` objects perform the reduction in a single process.  

## Methods using `C`-like arrays
:arrow_right: They start with an **uppercase letter**.  
:arrow_right: **vector variants** exist for these methods to send data of different size to different processes.  

---

# `mpi4py` - collective communications

.row[
.column.w48[
## .hcenter[Broadcast]
.hcenter.w65[ ![](images/broadcast.png)]

.vspace[]

## .hcenter[Gather]
.hcenter.w65[ ![](images/gather.png)]
]
.column.w48[
## .hcenter[Scatter]
.hcenter.w65[ ![](images/scatter.png)]

.vspace[]

## .hcenter[Allgather]
.hcenter.w65[ ![](images/allgather.png)]
]
]

---

# `mpi4py` - collective communications

## .hcenter[Alltoall]
.hcenter.w60[ ![](images/alltoall.png)]

---

# `mpi4py` - `Barrier`

:arrow_right: Use `Barrier()` to wait for all processes to reach a barrier.

## .hcenter[\[mpi4py/barrier.py\]]
```Python
#!/usr/bin/env python

import time
from mpi4py import MPI

time0 = time.time()

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

time.sleep(rank)
comm.Barrier()

time1 = time.time()

print('rank: %d, elapsed time: %fs' % (rank, time1 - time0))```

## .hcenter[Shell session]
```shell
$ mpiexec -np 4 mpi4py/barrier.py
rank: 1, elapsed time: 3.000270s
rank: 3, elapsed time: 3.000497s
rank: 0, elapsed time: 3.002678s
rank: 2, elapsed time: 3.002674s
```

:bulb: There exists a non-blocking version `Ibarrier()`.

---

# `mpi4py` - `Broadcast` a `Python` object

## .hcenter[\[mpi4py/broadcast_python.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = {'school' : 'ENSIIE', 'numbers': [2 + 1j, 3.1415]}
else:
    data = None

*data = comm.bcast(data, root = 0)

print("rank: %1d, recv: %s" % (rank, str(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/broadcast_python.py
rank: 0, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 1, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 9, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 5, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 8, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 4, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 2, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 3, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 6, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
rank: 7, recv: {'school': 'ENSIIE', 'numbers': [(2+1j), 3.1415]}
```

---

# `mpi4py` - `Scatter` a `Python` object

## .hcenter[\[mpi4py/scatter_python.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

if rank == 0:
    data = [(i + 1)**2 for i in range(size)]
else:
    data = None

*data = comm.scatter(data, root = 0)

print("rank: %1d, recv: %s" % (rank, str(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/scatter_python.py
rank: 0, recv: 1
rank: 9, recv: 100
rank: 6, recv: 49
rank: 2, recv: 9
rank: 4, recv: 25
rank: 7, recv: 64
rank: 3, recv: 16
rank: 8, recv: 81
rank: 1, recv: 4
rank: 5, recv: 36
```

---

# `mpi4py` - `Gather` a `Python` object

## .hcenter[\[mpi4py/gather_python.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

data = (rank + 1)**2

*data = comm.gather(data, root = 0)

print("rank: %1d, recv: %s" % (rank, str(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/gather_python.py
rank: 7, recv: None
rank: 8, recv: None
rank: 2, recv: None
rank: 6, recv: None
rank: 1, recv: None
rank: 5, recv: None
rank: 3, recv: None
rank: 4, recv: None
rank: 9, recv: None
rank: 0, recv: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
```

---

# `mpi4py` - `Allgather` a `Python` object

## .hcenter[\[mpi4py/allgather_python.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

data = (rank + 1)**2

*data = comm.allgather(data)

print("rank: %1d, recv: %s" % (rank, str(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 6 mpi4py/allgather_python.py
rank: 2, recv: [1, 4, 9, 16, 25, 36]
rank: 3, recv: [1, 4, 9, 16, 25, 36]
rank: 5, recv: [1, 4, 9, 16, 25, 36]
rank: 0, recv: [1, 4, 9, 16, 25, 36]
rank: 1, recv: [1, 4, 9, 16, 25, 36]
rank: 4, recv: [1, 4, 9, 16, 25, 36]
```

---

# `mpi4py` - `Alltoall` a `Python` object

## .hcenter[\[mpi4py/alltoall_python.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

data = [i + rank * size for i in range(size)]

*data = comm.alltoall(data)

print("rank: %1d, recv: %s" % (rank, str(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 3 mpi4py/alltoall_python.py
rank: 0, recv: [0, 3, 6]
rank: 1, recv: [1, 4, 7]
rank: 2, recv: [2, 5, 8]
```

---

# `mpi4py` - `Broadcast` a `numpy` object

## .hcenter[\[mpi4py/broadcast_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
  data = np.arange(100, dtype = 'i')
else:
  data = np.empty(100, dtype = 'i')

*comm.Bcast(data, root = 0)

print("rank: %1d, recv: %f" % (rank, np.linalg.norm(data)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/broadcast_numpy.py
rank: 0, recv: 573.018324
rank: 8, recv: 573.018324
rank: 2, recv: 573.018324
rank: 6, recv: 573.018324
rank: 1, recv: 573.018324
rank: 9, recv: 573.018324
rank: 5, recv: 573.018324
rank: 3, recv: 573.018324
rank: 7, recv: 573.018324
rank: 4, recv: 573.018324
```

---

# `mpi4py` - `Scatter` a `numpy` object

## .hcenter[\[mpi4py/scatter_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

sendbuf = None
if rank == 0:
  sendbuf = np.empty([size, 100], dtype = 'i')
  sendbuf.T[:, :] = range(size)

recvbuf = np.empty(100, dtype = 'i')

*comm.Scatter(sendbuf, recvbuf, root = 0)

print("rank: %1d, recv: %f" % (rank, np.linalg.norm(recvbuf)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/scatter_numpy.py
rank: 0, recv: 0.000000
rank: 5, recv: 50.000000
rank: 9, recv: 90.000000
rank: 1, recv: 10.000000
rank: 3, recv: 30.000000
rank: 8, recv: 80.000000
rank: 2, recv: 20.000000
rank: 4, recv: 40.000000
rank: 7, recv: 70.000000
rank: 6, recv: 60.000000
```

---

# `mpi4py` - `Gather` a `numpy` object

## .hcenter[\[mpi4py/gather_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

sendbuf = np.zeros(5, dtype = 'i') + rank
recvbuf = None
if rank == 0:
  recvbuf = np.empty([size, 5], dtype = 'i')

*comm.Gather(sendbuf, recvbuf, root = 0)

print("rank: %1d, recv: %s" % (rank, str(recvbuf)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 4 mpi4py/gather_numpy.py
rank: 1, recv: None
rank: 3, recv: None
rank: 2, recv: None
rank: 0, recv: [[0 0 0 0 0]
 [1 1 1 1 1]
 [2 2 2 2 2]
 [3 3 3 3 3]]
```

---

# `mpi4py` - `Allgather` a `numpy` object

## .hcenter[\[mpi4py/allgather_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

sendbuf = np.zeros(5, dtype = 'i') + rank
recvbuf = np.empty([size, 5], dtype = 'i')

*comm.Allgather(sendbuf, recvbuf, root = 0)

print("rank: %1d, recv: %s" % (rank, str(recvbuf)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 3 mpi4py/allgather_numpy.py
rank: 1, recv: [[0 0 0 0 0]
 [1 1 1 1 1]
 [2 2 2 2 2]]
rank: 0, recv: [[0 0 0 0 0]
 [1 1 1 1 1]
 [2 2 2 2 2]]
rank: 2, recv: [[0 0 0 0 0]
 [1 1 1 1 1]
 [2 2 2 2 2]]
```

---

# `mpi4py` - `Alltoall` a `numpy` object

## .hcenter[\[mpi4py/alltoall_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

sendbuf = np.arange(size, dtype = 'i') + rank * size
recvbuf = np.empty([size], dtype = 'i')

*comm.Alltoall(sendbuf, recvbuf)

print("rank: %1d, recv: %s" % (rank, str(recvbuf)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 5 mpi4py/alltoall_numpy.py
rank: 0, recv: [ 0  5 10 15 20]
rank: 2, recv: [ 2  7 12 17 22]
rank: 4, recv: [ 4  9 14 19 24]
rank: 3, recv: [ 3  8 13 18 23]
rank: 1, recv: [ 1  6 11 16 21]
```

---

# `mpi4py` - `Reduce` a `Python` object

## .hcenter[\[mpi4py/reduce_python.py\]]
```Python
#!/usr/bin/env python

import mpi4py.MPI as mpi

rank = mpi.COMM_WORLD.rank

*result = mpi.COMM_WORLD.reduce(rank, mpi.SUM, root = 0)

print("rank: %1d, y: %s" % (rank, str(result)))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/reduce_python.py
rank: 9, result: None
rank: 8, result: None
rank: 6, result: None
rank: 7, result: None
rank: 5, result: None
rank: 2, result: None
rank: 4, result: None
rank: 1, result: None
rank: 3, result: None
rank: 0, result: 45
```

---

# `mpi4py` - `Allreduce` a `Python` object

## .hcenter[\[mpi4py/allreduce_python.py\]]
```Python
#!/usr/bin/env python

import mpi4py.MPI as mpi

rank = mpi.COMM_WORLD.rank

*result = mpi.COMM_WORLD.allreduce(rank, mpi.SUM)

print("rank: %1d, result: %d" % (rank, result))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/allreduce_python.py
rank: 0, result: 45
rank: 1, result: 45
rank: 8, result: 45
rank: 3, result: 45
rank: 9, result: 45
rank: 2, result: 45
rank: 6, result: 45
rank: 4, result: 45
rank: 5, result: 45
rank: 7, result: 45
```


---

# `mpi4py` - `Reduce` a `numpy` object

## .hcenter[\[mpi4py/reduce_numpy.py\]]
```Python
#!/usr/bin/env python

import mpi4py.MPI as mpi
import numpy as np

rank = mpi.COMM_WORLD.rank

sendbuf = np.zeros([1], dtype = 'i') + rank

recvbuf = None
if rank == 0:
  recvbuf = np.empty([1], dtype = 'i')

*mpi.COMM_WORLD.Reduce(sendbuf, recvbuf, mpi.SUM, root = 0)

if rank == 0:
  print("rank: %1d, result: %d" % (rank, recvbuf[0]))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/reduce_numpy.py
rank: 0, result: 45
```

---

# `mpi4py` - `Allreduce` a `numpy` object

## .hcenter[\[mpi4py/allreduce_numpy.py\]]
```Python
#!/usr/bin/env python

import mpi4py.MPI as mpi
import numpy as np

rank = mpi.COMM_WORLD.rank

sendbuf = np.zeros([1], dtype = 'i') + rank
recvbuf = np.empty([1], dtype = 'i')

*mpi.COMM_WORLD.Allreduce(sendbuf, recvbuf, mpi.SUM)

print("rank: %1d, result: %d" % (rank, recvbuf[0]))
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/allreduce_numpy.py
rank: 0, result: 45
rank: 1, result: 45
rank: 8, result: 45
rank: 3, result: 45
rank: 9, result: 45
rank: 2, result: 45
rank: 6, result: 45
rank: 4, result: 45
rank: 5, result: 45
rank: 7, result: 45
```

:arrow_right: Some possible reduction operations: `SUM`, `PROD`, `MAX`, `MIN`, `MAXLOC`, `MINLOC`...

---

# `mpi4py` - `MPI-IO` `Write_at_all`

## .hcenter[\[mpi4py/mpiio_numpy.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

*fp = MPI.File.Open(comm, "/tmp/test_mpiio.dat", MPI.MODE_WRONLY | MPI.MODE_CREATE)

buffer = np.zeros(16, dtype=np.int8)
*buffer[0] = rank
*buffer[1:16] = [ord(i) for i in 'string in ascii']

offset = rank * buffer.nbytes
*fp.Write_at_all(offset, buffer)

*fp.Close()
```

## .hcenter[Shell session]
```shell
$ mpiexec -np 10 mpi4py/mpiio_numpy.py
$ hexdump -C /tmp/test_mpiio.dat
00000000  00 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000010  01 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000020  02 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000030  03 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000040  04 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000050  05 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000060  06 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000070  07 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000080  08 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
00000090  09 73 74 72 69 6e 67 20  69 6e 20 61 73 63 69 69  |.string in ascii|
000000a0
```

---

# `mpi4py` - `spawn` `MPI` processes

.row[
.column.w48[
## .hcenter[\[mpi4py/spawn_master.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy
import sys

nbprocs = 100

*comm = MPI.COMM_SELF.Spawn(sys.executable, args = ['spawn_slave.py'], maxprocs = nbprocs)

N = numpy.array(nbprocs, 'i')
comm.Bcast([N, MPI.INT], root = MPI.ROOT)
PI = numpy.array(0.0, 'd')
comm.Reduce(None, [PI, MPI.DOUBLE], op = MPI.SUM, root = MPI.ROOT)

print(PI)

*comm.Disconnect()
```
]
.column.w48[
## .hcenter[\[mpi4py/spawn_slave.py\]]
```Python
#!/usr/bin/env python

from mpi4py import MPI
import numpy

*comm = MPI.Comm.Get_parent()
size = comm.Get_size()
rank = comm.Get_rank()

N = numpy.array(0, dtype = 'i')
comm.Bcast([N, MPI.INT], root = 0)
h = 1.0 / N
s = 0.0
for i in range(rank, N, size):
  x = h * (i + 0.5)
  s += 4.0 / (1.0 + x**2)

PI = numpy.array(s * h, dtype='d')
comm.Reduce([PI, MPI.DOUBLE], None, op = MPI.SUM, root=0)

*comm.Disconnect()
```
]
]

## .hcenter[Shell session]
```shell
$ mpiexec -np 1 mpi4py/spawn_master.py
3.141600986923125
```

---

# `mpi4py` - conclusions

## To sum up
:arrow_right: Only a small subset of `MPI-2` capabilities has been presented.  
:arrow_right: `mpi4py` makes `MPI` methods easier to call.  
:warning: communicating generic `Python` objects is **convenient** but **not efficient**.  
:arrow_right: `MPI` method names and organization is highly reminiscent of `Fortran` or `C` code.  

.vspace[]

## To go further
:arrow_right: To write efficient `MPI` code, it is **mandatory** to dive into **lower-level programming**:
 * buffers
 * `C`-types
 * memory layout
 * network topology / latencies
 * IO caches
 * etc...

.vspace[]

## In case of serious `MPI` usage
* debugger: [`totalview`](https://www.roguewave.com/products-services/totalview) (commercial product)

---

# Bonus

.hcenter[[The list](https://www.top500.org/list/2017/11/)]

    </textarea>

    <script src="core/javascript/remark.js"></script>
    <script src="core/javascript/katex.min.js"></script>
    <script src="core/javascript/auto-render.min.js"></script>
    <script src="core/javascript/emojify.js"></script>
    <script src="core/javascript/mermaid.js"></script>
    <script src="core/javascript/term.js"></script>
    <script src="core/javascript/jquery-2.1.1.min.js"></script>
    <script src="core/javascript/extend-jquery.js"></script>
    <script src="core/javascript/cinescript.js"></script>
    <script src="core/javascript/gitgraph.js"></script>
    <script>

    // === Remark.js initialization ===
    var slideshow = remark.create({
      highlightStyle: 'monokai',
      countIncrementalSlides: false,
      highlightLines: true
    });

    // === Mermaid.js initialization ===
    mermaid.initialize({
      startOnLoad: false,
      cloneCssStyles: false,
      flowchart:{
        height: 50
      },
      sequenceDiagram:{
        width: 110,
        height: 30
      }
    });

    function initMermaid(s) {
      var diagrams = document.querySelectorAll('.mermaid');
      var i;
      for(i=0;i<diagrams.length;i++){
        if(diagrams[i].offsetWidth>0){
          mermaid.init(undefined, diagrams[i]);
        }
      }
    }

    slideshow.on('afterShowSlide', initMermaid);
    initMermaid(slideshow.getSlides()[slideshow.getCurrentSlideIndex()]);

    
    // === Emojify.js initialization ===
    emojify.run();

    // === Cinescript initialization ===
    $(document).ready(init_cinescripts);

    renderMathInElement(document.body,{delimiters: [{left: "$$", right: "$$", display: true}, {left: "\\(", right: "\\)", display: false}], ignoredTags: [] });

    </script>
  </body>
</html>

